{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "metadata = pd.read_csv('./data/metadata.csv')\n",
    "all_india = metadata[metadata['Division']=='India']\n",
    "\n",
    "for i,row in all_india.iterrows():\n",
    "    all_india.loc[i, [\"Local_lat\"]] = float(row[\"Local_lat\"])\n",
    "    \n",
    "north_indian_states = {\n",
    "       'Chhattisgarh',\n",
    "       'Maharashtra', 'Malwa',\n",
    "       'Chota Nagpur Plateau',\n",
    "       'Bihar/ Uttar Pradesh', 'Uttar Pradesh', 'Varanasi, Uttar Pradesh',\n",
    "       'Rajasthan', 'Gujarat',\n",
    "       'Gujarat/ Madhya Pradesh/ Maharashtra/ Rajasthan'\n",
    "}\n",
    "\n",
    "north_india = pd.DataFrame()\n",
    "south_india = pd.DataFrame()\n",
    "\n",
    "for i,row in all_india.iterrows():\n",
    "    if row[\"Area/Kingdom\"] in north_indian_states:\n",
    "        north_india = north_india.append(all_india.loc[i])\n",
    "    else:\n",
    "        south_india = south_india.append(row)\n",
    "        \n",
    "north_india = north_india.rename(columns={'C-id':'canto_coding_id'})\n",
    "south_india = south_india.rename(columns={'C-id':'canto_coding_id'})\n",
    "\n",
    "north_india.to_csv('./data/north-india.csv')\n",
    "south_india.to_csv('./data/south-india.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classfication_metadata = pd.read_csv('./data/classification_metadata.csv',encoding='latin1')\n",
    "classfication_metadata[classfication_metadata['Culture']=='Dhurwa Gond']['Lang_Family'].values[0]\n",
    "language_map = {\n",
    "    'Dravidian, Central Dravidian':'Dravidian',\n",
    "    'Dravidian, South Dravidian, Gondi, Koya': 'Dravidian',\n",
    "    'Dravidian, South Dravidian, Gondi' :'Dravidian',\n",
    "    'Dravidian, South Dravidian': 'Dravidian',\n",
    "    'Dravidian, North Dravidian, Kurux-Malto, Kurux':'Dravidian',\n",
    "    'Indo-European, Indo-Iranian, Indo-Aryan, Western Hindi, Hindustani': 'Indo-European',\n",
    "    'Austroasiatic, Mundaic, North Munda': 'Austroasiatic',\n",
    "    'Austroasiatic, Mundaic, Mundaric, Ho-Mundari':'Austroasiatic',\n",
    "    'Austroasiatic, Mundaic, North Munda, Mundaric':'Austroasiatic',\n",
    "    'Austroasiatic, Mundaic, North Munda, Kherwarian':'Austroasiatic',\n",
    "    'Indo-European, Indo-Iranian, Indo-Aryan, Bihari, Western Magadhan, Bhojpuric':'Indo-European',\n",
    "    'Indo-European, Indo-Iranian, Indo-Aryan, Gujarati-Rajasthani':'Indo-European',\n",
    "    'Indo-European, Indo-Iranian, Indo-Aryan, Gujarati-Rajasthani, Gujaratic':'Indo-European',\n",
    "    'Indo-European, Indo-Iranian, Indo-Aryan, Subcontinental Central Indo-Aryan':'Indo-European',\n",
    "    'Indo-European, Indo-Iranian, Indo-Aryan, Rajasthani':'Indo-European',\n",
    "    'Dravidian, Central Dravidian, Kolami-Naiki':'Dravidian',\n",
    "    'Indo-European, Indo-Iranian, Indo-Aryan, Marathic':'Indo-European',\n",
    "    'Dravidian, South Dravidian, Tamil-Kannada, Tamil-Malayalam, Tamiloid':'Dravidian',\n",
    "    'Dravidian, South Dravidian, Teluguic':'Dravidian',\n",
    "    'Dravidian, South Dravidian, Gondi':'Dravidian',\n",
    "    'Dravidian, Parji-Ollari-Gadaba, Ollari-Gadaba':'Dravidian',\n",
    "    'Austroasiatic, Mundaic, Sora-Juray-Gorum':'Austroasiatic',\n",
    "    'Dravidian, South Dravidian, Konda-Kui':'Dravidian',\n",
    "    'Austroasiatic, Mundaic, South Munda, Gutob-Remo':'Austroasiatic',\n",
    "    'Indo-European, Indo-Iranian, Indo-Aryan, Oriya-Gauda-Kamrupa, Macro-Oriya':'Indo-European',\n",
    "    'Indo-European, Indo-Iranian, Indo-Aryan, Indo-Aryan Eastern Zone':'Indo-European',\n",
    "    'Indo-European, Italic, Latinic, Romance, Western Romance, Galician Romance, Macro-Portuguese':'Indo-European',\n",
    "    'Dravidian, South Dravidian, Tamil-Kannada, Tamil-Malayalam, Malayamoid':'Dravidian',\n",
    "    'Dravidian, South Dravidian, Tamil-Kannada, Tamil-Toda':'Dravidian',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "north_india = pd.read_csv('./data/north-india.csv')\n",
    "south_india = pd.read_csv('./data/south-india.csv')\n",
    "metadata = pd.read_csv('./data/metadata.csv')\n",
    "with open('./output/output.json') as json_file:\n",
    "    codebook = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tracks is 208\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of tracks is\",len(north_india)+len(south_india))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw from Google Sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pickle\n",
    "import os.path\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "\n",
    "SCOPES = ['https://www.googleapis.com/auth/spreadsheets.readonly']\n",
    "\n",
    "# Reference: https://developers.google.com/sheets/api/quickstart/python\n",
    "def read_google_sheets(SPREADSHEET_ID, RANGE_NAME, HEADER_RANGE):\n",
    "    creds = None\n",
    "    # autogenerated\n",
    "    if os.path.exists('token.pickle'):\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'credentials.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        with open('token.pickle', 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "\n",
    "    service = build('sheets', 'v4', credentials=creds)\n",
    "\n",
    "    sheet = service.spreadsheets()\n",
    "    result = sheet.values().get(spreadsheetId=SPREADSHEET_ID,\n",
    "                                range=RANGE_NAME).execute()\n",
    "    \n",
    "    header = sheet.values().get(spreadsheetId=SPREADSHEET_ID,\n",
    "                               range=HEADER_RANGE).execute()\n",
    "    \n",
    "    header_values = header.get('values', [])\n",
    "    values = result.get('values', [])\n",
    "    \n",
    "    return values, header_values\n",
    "\n",
    "SPREADSHEET_ID = '1AjynK9mMQTw58B_B8b_ZIip3fyUm-aoV7Pp21HziBb0'\n",
    "RANGE_NAME = 'canto_codings!A2:AT'\n",
    "HEADER_RANGE = 'canto_codings!A1:AT1'\n",
    "\n",
    "data, header = read_google_sheets(SPREADSHEET_ID, RANGE_NAME, HEADER_RANGE)\n",
    "\n",
    "canto_codings = pd.DataFrame(data, columns = header[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_canto_features(canto_coding_id):\n",
    "    for i,row in canto_codings.iterrows():\n",
    "        if(str(canto_coding_id) == row['canto_coding_id']):\n",
    "            return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_canto_metadata(canto_coding_id):\n",
    "    for i, row in metadata.iterrows():\n",
    "        if(str(canto_coding_id) == row['C-id']):\n",
    "            return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_display_code(line, binary_code):\n",
    "    line = codebook['line_'+str(line)]\n",
    "    for encoding in line:\n",
    "        if encoding['code'] == str(binary_code):\n",
    "            return encoding['display_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_language_family(culture):\n",
    "    indian_metadata = classfication_metadata[classfication_metadata['Culture']==culture]\n",
    "    try:\n",
    "        language = indian_metadata['Lang_Family'].values[0]\n",
    "        people = indian_metadata['People'].values[0]\n",
    "        if \"(Ethnic Peoples)\" in people:\n",
    "            people = \"Tribal\"\n",
    "        else:\n",
    "            people = \"Non-Tribal\"\n",
    "    except IndexError as e:\n",
    "        return \"Dravidian\", \"Tribal\"\n",
    "    return language_map[language], people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Output Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"canto_coding_id\", \"region\", \"division\",\"subregion\", \"area_kingdom\", \"culture\", \"language\", \"people\",\"song\",\"lat\", \"lng\"]\n",
    "\n",
    "for i in range(37):\n",
    "    columns.append(\"cv_\"+str(i+1))\n",
    "    \n",
    "north_india_full = pd.DataFrame(columns=columns)\n",
    "south_india_full = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(input_matrix, output_matrix):\n",
    "    for i, row in input_matrix.iterrows():\n",
    "        canto_coding_id = row['canto_coding_id']\n",
    "        culture = row['Culture']\n",
    "        meta = get_canto_metadata(int(canto_coding_id))\n",
    "        lat = meta['Local_lat']\n",
    "        lng = meta['Local_long']\n",
    "        region = meta['Region']\n",
    "        division = meta['Division']\n",
    "        subregion = meta['Subregion']\n",
    "        area = meta['Area/Kingdom']\n",
    "        song = meta['Song']\n",
    "        language,people = get_language_family(culture)\n",
    "        canto = find_canto_features(canto_coding_id)\n",
    "        canto_data = []\n",
    "        for i in range(37):\n",
    "            canto_data.append(get_display_code(i+1, canto[\"cv_\"+str(i+1)]))\n",
    "        new_row = pd.DataFrame([[\n",
    "            canto_coding_id,\n",
    "            region,\n",
    "            division,\n",
    "            subregion,\n",
    "            area,\n",
    "            culture,\n",
    "            language,\n",
    "            people,\n",
    "            song,\n",
    "            lat,\n",
    "            lng,\n",
    "            canto_data[0],\n",
    "            canto_data[1],\n",
    "            canto_data[2],\n",
    "            canto_data[3],\n",
    "            canto_data[4],\n",
    "            canto_data[5],\n",
    "            canto_data[6],\n",
    "            canto_data[7],\n",
    "            canto_data[8],\n",
    "            canto_data[9],\n",
    "            canto_data[10],\n",
    "            canto_data[11],\n",
    "            canto_data[12],\n",
    "            canto_data[13],\n",
    "            canto_data[14],\n",
    "            canto_data[15],\n",
    "            canto_data[16],\n",
    "            canto_data[17],\n",
    "            canto_data[18],\n",
    "            canto_data[19],\n",
    "            canto_data[20],\n",
    "            canto_data[21],\n",
    "            canto_data[22],\n",
    "            canto_data[23],\n",
    "            canto_data[24],\n",
    "            canto_data[25],\n",
    "            canto_data[26],\n",
    "            canto_data[27],\n",
    "            canto_data[28],\n",
    "            canto_data[29],\n",
    "            canto_data[30],\n",
    "            canto_data[31],\n",
    "            canto_data[32],\n",
    "            canto_data[33],\n",
    "            canto_data[34],\n",
    "            canto_data[35],\n",
    "            canto_data[36],\n",
    "        ]], columns = columns)\n",
    "        output_matrix = output_matrix.append(new_row)\n",
    "    return output_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run data conversion. This will take a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "north_india_full = main(north_india, north_india_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "south_india_full = main(south_india, south_india_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write data to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "north_india_full.to_csv('./data/north_india_full.csv', index=False)\n",
    "south_india_full.to_csv('./data/south_india_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = []\n",
    "for i in range(len(north_india_full)):\n",
    "    region.append(\"North\")\n",
    "for i in range(len(south_india_full)):\n",
    "    region.append(\"South\")\n",
    "\n",
    "frames = [north_india_full, south_india_full]\n",
    "all_india_full = pd.concat(frames)\n",
    "all_india_full[\"region\"] = region\n",
    "all_india_full = all_india_full[all_india_full[\"culture\"]!=\"Portuguese Goa\"]\n",
    "all_india_full.to_csv('./data/all_india_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dravidian        101\n",
       "Indo-European     55\n",
       "Austroasiatic     51\n",
       "Name: language, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_india_full = pd.read_csv('./data/all_india_full.csv')\n",
    "all_india_full['language'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tribal        150\n",
       "Non-Tribal     57\n",
       "Name: people, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_india_full['people'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = 0\n",
    "for i, row in all_india_full.iterrows():\n",
    "    if row['language'] != 'Austroasiatic' and row['people']=='Non-Tribal':\n",
    "        c+=1\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important: Change the file's content so that Tamil People are coded as dravidians!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data for AMOVA for Indian Sample in R\n",
    "\n",
    "## Removing Cultures With Only One Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance_matrix\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samples Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_india_full = pd.read_csv('./data/all_india_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-142-0518c67386b4>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_india_full['cv_1'][196] = 2\n"
     ]
    }
   ],
   "source": [
    "# This value was missing. \n",
    "all_india_full['cv_1'][196] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_india_full['Number of Samples'] = all_india_full.groupby('culture')['culture'].transform('count')\n",
    "no_singles = all_india_full[all_india_full['Number of Samples']!=1]\n",
    "singles = all_india_full[all_india_full['Number of Samples']==1]\n",
    "filtered_df = all_india_full[(all_india_full['language']!='Austroasiatic') & (all_india_full['people']!='Tribal')]\n",
    "\n",
    "filtered_df.to_csv('./data/filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_columns = filtered_df['culture'].unique()\n",
    "sample_columns = np.insert(sample_columns, 0, 'canto_coding_id', axis=0)\n",
    "samples = pd.DataFrame(columns=sample_columns)\n",
    "## fill this up with zeros\n",
    "\n",
    "for i, row in filtered_df.iterrows():\n",
    "    samples.loc[i, row['culture']] = 1\n",
    "    samples.loc[i, 'canto_coding_id'] = row[\"canto_coding_id\"]\n",
    "    \n",
    "samples = samples.fillna(0)\n",
    "samples = samples.reset_index()\n",
    "samples = samples.drop(columns=['index'])\n",
    "\n",
    "samples.to_csv('./data/samples.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structures Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_list = []\n",
    "for i in range(len(sample_columns)-1):\n",
    "    structure_list.append(filtered_df[filtered_df['culture']==sample_columns[i+1]]['language'].all())\n",
    "structure = pd.DataFrame(structure_list, columns = ['language'])\n",
    "structure = structure['language'].replace('Indo-European', 'Indo-Aryan')\n",
    "structure.to_csv('./data/structure.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_india_full['cv_1'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare AMOVA Data for Global Sample in R\n",
    "## Remove cultures with Only One Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "canto_codings['Number of Samples'] = canto_codings.groupby('Culture')['Culture'].transform('count')\n",
    "singles = canto_codings[canto_codings['Number of Samples']!=1]\n",
    "singles.to_csv('./data/global_no_singles.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appending regions to the canto_codings dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "canto_codings['region'] = metadata['Region']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samples Dataframe\n",
    "### This will take a while to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_columns = singles['Culture'].unique()\n",
    "sample_columns = np.insert(sample_columns, 0, 'canto_coding_id', axis=0)\n",
    "samples = pd.DataFrame(columns=sample_columns)\n",
    "## fill this up with zeros\n",
    "\n",
    "for i, row in singles.iterrows():\n",
    "    samples.loc[i, row['Culture']] = 1\n",
    "    samples.loc[i, 'canto_coding_id'] = row[\"canto_coding_id\"]\n",
    "samples = samples.fillna(0)\n",
    "samples.to_csv('./data/global_samples.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Structures Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_list = []\n",
    "for i in range(len(sample_columns)-1):\n",
    "    structure_list.append(canto_codings[canto_codings['Culture']==sample_columns[i+1]]['region'].all())\n",
    "structure = pd.DataFrame(structure_list, columns = ['region'])\n",
    "structure.to_csv('./data/global_structure.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Indo-European    28\n",
       "Dravidian        19\n",
       "Name: language, dtype: int64"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df['language'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
